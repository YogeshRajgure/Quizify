{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from ollama import chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The color of the sky, or its transparency, is primarily due to a combination of factors. When sunlight reaches Earth's atmosphere, it undergoes several refractive changes. The ozone layer in Earth's upper atmosphere absorbs ultraviolet (UV) light that would otherwise cause the blue color we see in the sky. Additionally, as sunlight passes through Earth's atmosphere, its wavelength becomes longer and is refracted more, which effectively spreads out the colors of the spectrum into a wide range of visible hues. This phenomenon is known as dispersion.\n",
      "\n",
      "Therefore, the sky appears blue because the light has been scattered by particles in the atmosphere (ozone, water, and other substances), giving it a more colorful appearance.\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model = 'deepseek-r1:1.5b',\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    "    # other params ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, so I need to translate the sentence \"I love programming.\" into French. Let\\'s see... First, the main verb is \"love,\" which in French can be \"appel\" or \"vendre à.\" But in this context, it\\'s probably more about expressing a personal preference for something.\\n\\nNext, \"programming\" has multiple translations. The most common one is \"programmation.\" So putting it together, \"I love programming\" would read as \"Je_apple le programmet.\"\\n</think>\\n\\nJe.apple le programmet.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-01-28T15:57:44.6970705Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6462205000, 'load_duration': 32156100, 'prompt_eval_count': 20, 'prompt_eval_duration': 446000000, 'eval_count': 115, 'eval_duration': 5669000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c5895dec-76f3-4cb9-8d96-6dd41c7bb5e5-0', usage_metadata={'input_tokens': 20, 'output_tokens': 115, 'total_tokens': 135})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "llm.invoke(messages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_llm = ChatOllama(model = 'deepseek-r1:7b', format=\"json\")\n",
    "messages = [\n",
    "    (\"human\", \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. Respond using JSON only.\"),\n",
    "]\n",
    "a = llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to create a JSON query that retrieves weather data from a specific location at a particular time of day. The user wants it structured with keys 'location' and 'time_of_day'. \n",
      "\n",
      "First, I'll think about the JSON structure. Since it's a list containing one item (the weather object), I can use an array. Each item will have two keys: location as \"latitude longitude\" and time_of_day in \"hour minute\".\n",
      "\n",
      "I remember that weather data might come from APIs like OpenWeatherMap or similar services. But since this is just the query, I don't need to include the API request here.\n",
      "\n",
      "Putting it all together, the JSON should look like an array with one object, each key separated by commas and proper quotes. The keys are in double quotes so they're treated as strings without escaping any special characters.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"location\": [\"latitude\", \"longitude\"],\n",
      "    \"time_of_day\": [\"hour\", \"minute\"]\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_context = \"\"\"\n",
    "You are an assistant that generates interview quiz questions from resume or job description provided by the user to test their knowledge on those skills.\n",
    "Based on the text provided by user, create a JSON object with 10 multiple-choice questions to test the user for their skills,\n",
    "each having 4 options and one correct answer.\n",
    "Output format:\n",
    "{\n",
    "    \"title\": \"Quiz Title\",\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"question\": \"What is the capital of France?\",\n",
    "            \"choices\": [\n",
    "                \"Paris\",\n",
    "                \"London\",\n",
    "                \"Berlin\",\n",
    "                \"Madrid\"\n",
    "            ],\n",
    "            \"answer\": \"Paris\"\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "do not format as ```json\\n{ }```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_text = \"\"\"\n",
    "Titans: Learning to Memorize at Test Time\n",
    "Ali Behrouz†\n",
    ", Peilin Zhong†\n",
    ", and Vahab Mirrokni†\n",
    "†\n",
    "Google Research\n",
    "{alibehrouz, peilinz, mirrokni}@google.com\n",
    "Abstract\n",
    "Over more than a decade there has been an extensive research effort of how effectively utilize recurrent models and\n",
    "attentions. While recurrent models aim to compress the data into a fixed-size memory (called hidden state), attention allows\n",
    "attending to the entire context window, capturing the direct dependencies of all tokens. This more accurate modeling\n",
    "of dependencies, however, comes with a quadratic cost, limiting the model to a fixed-length context. We present a new\n",
    "neural long-term memory module that learns to memorize historical context and helps an attention to attend to the\n",
    "current context while utilizing long past information. We show that this neural memory has the advantage of a fast\n",
    "parallelizable training while maintaining a fast inference. From a memory perspective, we argue that attention due to its\n",
    "limited context but accurate dependency modeling performs as a short-term memory, while neural memory due to its\n",
    "ability to memorize the data, acts as a long-term, more persistent, memory. Based on these two modules, we introduce\n",
    "a new family of architectures, called Titans, and present three variants to address how one can effectively incorporate\n",
    "memory into this architecture. Our experimental results on language modeling, common-sense reasoning, genomics,\n",
    "and time series tasks show that Titans are more effective than Transformers and recent modern linear recurrent models.\n",
    "They further can effectively scale to larger than 2M context window size with higher accuracy in needle-in-haystack tasks\n",
    "compared to baselines.\n",
    "1 Introduction\n",
    "“The true art of memory is the art of attention!\"\n",
    "— Samuel Johnson, 1787\n",
    "T\n",
    "ransformers, pure attention-based architectures (Vaswani et al. 2017), have been firmly established as state-of\u0002the-art models in sequence modeling, mainly due to their in-context learning and ability to learn at scale (Kaplan\n",
    "et al. 2020). The primary building blocks of Transformers–attention modules—function as associative memory\n",
    "blocks (Bietti et al. 2024), where they learn to store key-value associations and retrieve them by computing pairwise\n",
    "similarity between queries (i.e., search signals) and keys (i.e., contexts). Accordingly, by design, the output of a Transformer\n",
    "is exclusively conditioned on the direct dependencies of tokens in the current context window. This accurate modeling of\n",
    "dependencies, however, comes with quadratic time and memory complexity in terms of the context length. In complex\n",
    "real-world tasks (e.g., language modeling (N. F. Liu et al. 2024), video understanding (C.-Y. Wu et al. 2019), long-term time\n",
    "series forecasting (H. Zhou et al. 2021)), the context window can become extremely large, making the applicability of\n",
    "Transformers challenging in these downstream tasks.\n",
    "To overcome the scalability issue of Transformers, recent studies aim to design different variants of linear Transform\u0002ers (Kacham, Mirrokni, and P. Zhong 2024; Katharopoulos et al. 2020; S. Yang, B. Wang, Shen, et al. 2024), where softmax is\n",
    "replaced by a kernel function in the attention (see §2.1 for details), resulting in a significant drop in memory consumption.\n",
    "Despite efficiency and the ability to scale to longer context, linear Transformers do not show competitive performance\n",
    "compared to Transformers as the kernel trick makes the model a linear recurrent network, in which the data is compressed\n",
    "into a matrix-valued states (Katharopoulos et al. 2020). This, however, brings a contradictory fact about linear recurrent (or\n",
    "linear Transformers) models: On one hand, we use these linear models to enhance scalability and efficiency (linear vs.\n",
    "quadratic complexity), whose advantages is appeared for very long context; On the other hand, a very long context cannot\n",
    "be properly compressed in a small vector-valued or matrix-valued states (S. Wang 2024).\n",
    "1\n",
    "arXiv:2501.00663v1 [cs.LG] 31 Dec 2024\n",
    "Furthermore, beyond efficiency, most existing architectures–ranging from Hopfield Networks (Hopfield 1982) to LSTMs (Jür\u0002gen Schmidhuber and Hochreiter 1997) and Transformers (Vaswani et al. 2017)–face challenges when dealing with general\u0002ization, length extrapolation, and/or reasoning (Anil et al. 2022; Qin, Y. Zhong, and Deng 2024), all of which are inseparable\n",
    "parts of many hard real-world tasks. Although these architectures draw inspiration from the human brain, each of which\n",
    "are missing: (1) a crucial component for learning process—such as short-term memory, long-term memory, meta-memory,\n",
    "attending to current context, etc. (Cowan 2008); (2) how these components are interconnected systems that can operate\n",
    "independently; and/or (3) the ability to actively learn from data and memorize the abstraction of past history. We argue\n",
    "that in an effective learning paradigm, similar to human brain, there are distinct yet interconnected modules, each of which\n",
    "is responsible for a component crucial to the learning process.\n",
    "Memory Perspective\n",
    "Memory is a fundamental mental process and is an inseparable component of human learning (Terry 2017). Without\n",
    "a properly functioning memory system, humans and animals would be restricted to basic reflexes and stereotyped\n",
    "behaviors. Accordingly, memory has been the inspiration for many seminal research in machine learning literature; e.g.,\n",
    "Hopfield Networks (Hopfield 1982), LSTMs (Jürgen Schmidhuber and Hochreiter 1997), and Transformers (Vaswani et al.\n",
    "2017).\n",
    "Taking inspiration from the common definitions of memory and learning in neuropsychology literature (Okano, Hirano,\n",
    "and Balaban 2000), most existing architectures consider memory as a neural update caused by an input, and define learning\n",
    "as a process for acquiring effective and useful memory, given an objective. In this perspective, Recurrent Neural Networks\n",
    "(RNNs) (Williams and Zipser 1989) can be defined as models with a vector-valued memory module M (also called hidden\n",
    "state) with two main steps: Given a new input 𝑥𝑡 at time 𝑡, the model (1) updates the memory using a function 𝑓 (M𝑡−1, 𝑥𝑡)\n",
    "(with compression); and (2) retrieves the corresponding memory of input using a function 𝑔(M𝑡\n",
    ", 𝑥𝑡) (see §2.1 for details).\n",
    "Similarly, Transformers can be seen as architectures with a growing memory and two similar steps. That is, the pair of key\n",
    "and value matrices acts as the model’s memory, and the model: (1) updates the memory by appending the key and value to\n",
    "the memory (without compression), and (2) retrieves query vectors’ corresponding memory by finding the similarity of\n",
    "query and key vectors, which is then used to weight the value vectors for the output.\n",
    "This perspective, can help us better understand existing paradigms, their critical differences, and design more effective\n",
    "architectures. For example, the main difference between Transformers (Vaswani et al. 2017) and linear Transform\u0002ers (Katharopoulos et al. 2020) is the memory structure as well as the memory updating step, in which linear Transformers\n",
    "compress the historical data into a fixed-size matrix-valued memory while Transformers keep all historical data (within\n",
    "the context length) without any compression. While both linear Transformers and linear RNNs (including state space\n",
    "models) compress the information in memory update step, the critical difference lies in the structure of the memory,\n",
    "where linear RNNs (vs. linear Transformers) use a vector-valued memory (vs. matrix-valued memory). Therefore, this\n",
    "perspective motivates us to ask: (Q1) What constitute a good structure for the memory? (Q2) What is a proper memory\n",
    "update mechanism? and (Q3) What is a good memory retrieval process?\n",
    "Revisiting our understanding of human memory, it is neither a unitary process nor it serves a single function (Cowan\n",
    "2008). In fact, memory is a confederation of systems–e.g., short-term, working, and long-term memory–each serving a\n",
    "different function with different neural structures, and each capable of operating independently (Willingham 1997). This\n",
    "fact motivates us to ask: (Q4) How to design an efficient architecture that incorporates different interconnected memory\n",
    "modules. Finally, storing a memory is a neural process that requires to encode and store the abstraction of the past. It can\n",
    "be over-simplification to assume a single vector or a matrix, whose parameters are encoding the data in a linear manner,\n",
    "are enough for storing long-term history. (Q5) Is a deep memory module needed to effectively store/remember long\n",
    "past?\n",
    "Contributions and Roadmap\n",
    "In this paper, we aim to answer the above five questions by designing a long-term neural memory module, that can\n",
    "efficiently and effectively learn to memorize at test time. Building upon its design, we discuss how it can be incorporated\n",
    "into an architecture.\n",
    "Neural Memory (§3). We present a (deep) neural long-term memory that (as a meta in-context model) learns how to\n",
    "memorize/store the data into its parameters at test time. Inspired by human long-term memory system (Mandler 2014),\n",
    "2\n",
    "we design this memory module so an event that violates the expectations (being surprising) is more memorable. To this\n",
    "end, we measure the surprise of an input with the gradient of the neural network with respect to the input in associative\n",
    "memory loss (see §3.1 for details). To better handle the limited memory, we present a decaying mechanism that consider the\n",
    "proportion of memory size and the amount of data surprise, resulting in better memory management. We show that this\n",
    "decay mechanism is in fact the generalization of forgetting mechanism in modern recurrent models (Dao and Gu 2024; Gu\n",
    "and Dao 2024; S. Yang, Kautz, and Hatamizadeh 2024). Interestingly, we find that this mechanism is equivalent to optimizing\n",
    "a meta neural network with mini-batch gradient descent, momentum, and weight decay. Building upon tensorizing\n",
    "mini-batch gradient descent to use more matmul operations (Yu Sun et al. 2024), we present a fast and parallelizable\n",
    "algorithm to train our deep neural long-term memory.\n",
    "Titans Architectures (§4). After designing the long-term neural memory, an important remaining question is how to\n",
    "effectively and efficiently incorporate memory into a deep learning architecture. We present Titans, a family of deep models\n",
    "that consists of three hyper-heads: (1) Core: this module consists of the short-term memory, and is responsible for the main\n",
    "flow of processing the data (we use attention with limited window size); (2) Long-term Memory: this branch is our neural\n",
    "long-term memory module that is responsible to store/remember long past; (3) Persistent Memory: this is a set of learnable\n",
    "but date-independent parameters that encodes the knowledge about a task. Finally, as a proof of concept, we present three\n",
    "variants of Titans, in which we incorporate memory as: (i) a context, (ii) a layer, and (iii) a gated branch.\n",
    "Experimental Results (§5). We perform experimental evaluations on language modeling, commonsense reasoning, recall\u0002intensive, needle in haystack, time series forecasting, and DNA modeling tasks. We observe that our Titan architecture\n",
    "outperforms all modern recurrent models as well as their hybrid variants (combining with sliding-window attention) across\n",
    "a comprehensive set of benchmarks. Furthermore, Titans outperforms Transformers with the same context window, and\n",
    "show competitive performance with Transformers that use the entire context. This results are achieved while, contrary to\n",
    "Transformers, Titans scale to larger than 2M context window size.\n",
    "2 Preliminaries\n",
    "I\n",
    "n this section, we discuss the notation and some background concepts that we use though the paper. We let\n",
    "𝑥 ∈ R\n",
    "𝑁 ×𝑑in be the input, M be a neural network (neural memory module), Q, K, V be the query, key and value\n",
    "of the attention mechanism, and M be the attention mask. When segmenting the sequence, we use S\n",
    "(𝑖)\n",
    "to refer to\n",
    "the 𝑖-th segment. Through the paper, we abuse the notation and use subscripts to refer to a specific element of a matrix,\n",
    "vector, or segments. For example, we let S\n",
    "(𝑖)\n",
    "𝑗\n",
    "be the 𝑗-th token in the 𝑖-th segment. The only exception is subscripts with 𝑡,\n",
    "which we reserved to index recurrence over time, or the state of a neural network at time 𝑡. Given a neural network N and\n",
    "a data sample 𝑥, we use N (𝑥) (resp. N∗\n",
    "(𝑥)) to refer to the forward pass with (resp. without) weight adjustment. Also, we\n",
    "abuse the notation and use N (𝑘)\n",
    "to refer to the 𝑘-th layer of the neural network. In the following, we first, discuss the\n",
    "backgrounds for attention and its efficient variants followed by a review of modern linear RNNs. Finally, we discuss a\n",
    "memory perspective of these architectures that motivates us to design Titans.\n",
    "2.1 Backgrounds\n",
    "Attention. Transformers (Vaswani et al. 2017) as the de facto backbone for many deep learning models are based on\n",
    "attention mechanism. Given input 𝑥 ∈ R\n",
    "𝑁 ×𝑑in , causal attention computes output y ∈ R\n",
    "𝑁 ×𝑑in based on softmax over input\n",
    "dependent key, value, and query matrices:\n",
    "Q = 𝑥WQ, K = 𝑥WK, V = 𝑥WV, (1)\n",
    "y𝑖 =\n",
    "∑︁\n",
    "𝑖\n",
    "𝑗=1\n",
    "exp \u0010\n",
    "Q⊤\n",
    "𝑖 K𝑗 /\n",
    "√\n",
    "𝑑in\u0011\n",
    "V𝑗\n",
    "Í𝑖\n",
    "ℓ=1\n",
    "exp \u0010\n",
    "Q⊤\n",
    "𝑖 Kℓ/\n",
    "√\n",
    "𝑑in\u0011, (2)\n",
    "where WQ, WK, and WV ∈ R\n",
    "𝑑in×𝑑in are learnable parameters. Despite the power and effectiveness in recall, transformers\n",
    "need at least 𝑁 × 𝑑 operators to calculate the output, resulting in larger memory consumption and lower-throughput for\n",
    "longer sequences.\n",
    "Efficient Attentions. To improve the memory consumption and throughput of softmax attention for longer sequences,\n",
    "various studies focused on I/O aware implementations of attention (Dao 2024; Dao, D. Fu, et al. 2022), designing more\n",
    "3\n",
    "efficient attention mechanisms by sparsifying the attention matrix (B. Chen et al. 2021; Choromanski et al. 2021; Dai et al.\n",
    "2019), approximating the softmax (Arora et al. 2024), or developing kernel-based (linear) attentions (Aksenov et al. 2024;\n",
    "Kacham, Mirrokni, and P. Zhong 2024; Schlag, Irie, and Jürgen Schmidhuber 2021; S. Yang, B. Wang, Shen, et al. 2024). In\n",
    "this part, we focus on the later, i.e., linear attentions, where the softmax in standard attention is replaced with an alternative\n",
    "kernel function 𝜙 (., .), such that 𝜙 (𝑥, 𝑦) = 𝜙 (𝑥)𝜙 (𝑦). Accordingly, the attention can be written as:\n",
    "y𝑖 =\n",
    "∑︁\n",
    "𝑖\n",
    "𝑗=1\n",
    "𝜙 (𝑄\n",
    "⊤\n",
    "𝑖 𝐾𝑗)\n",
    "Í𝑖\n",
    "ℓ=1 𝜙 (𝑄\n",
    "⊤\n",
    "𝑖\n",
    "𝐾ℓ)\n",
    "𝑉𝑗 =\n",
    "∑︁\n",
    "𝑖\n",
    "𝑗=1\n",
    "𝜙 (𝑄𝑖)\n",
    "⊤𝜙 (𝐾𝑗)\n",
    "Í𝑖\n",
    "ℓ=1 𝜙 (𝑄𝑖)\n",
    "⊤𝜙 (𝐾ℓ)\n",
    "𝑉𝑗 =\n",
    "𝜙 (𝑄𝑖)\n",
    "⊤ Í𝑖\n",
    "𝑗=1 𝜙 (𝐾𝑗)𝑉𝑗\n",
    "𝜙 (𝑄𝑖)\n",
    "⊤\n",
    "Í𝑖\n",
    "ℓ=1 𝜙 (𝐾ℓ)\n",
    ", (3)\n",
    "resulting in a higher-throughput as terms Í𝑖\n",
    "𝑗=1 𝜙 (𝐾𝑗) and Í𝑖\n",
    "ℓ=1 𝜙 (𝐾ℓ) are re-using in each step. When choosing the kernel\n",
    "as identity matrix (Yutao Sun et al. 2023), the above formulation can also be written in a recurrent format:\n",
    "M𝑡 = M𝑡−1 + 𝐾\n",
    "⊤\n",
    "𝑡 𝑉𝑡\n",
    ", (4)\n",
    "y𝑡 = 𝑄𝑡M𝑡\n",
    ", (5)\n",
    "which allows efficient inference for linear attentions.\n",
    "Modern Linear Models and Their Memory Perspective. As discussed earlier, one can define learning as a process for\n",
    "acquiring effective and useful memory. Building upon this, one can see the hidden state of Recurrent Neural Networks\n",
    "(RNNs) as a memory unit, which the model aims to compress the information into. Accordingly, in a general form of\n",
    "recurrent neural network, the hidden state can be treated as a memory unit and the recurrence process can be split into the\n",
    "read and write operations in the memory unit. That is, we let 𝑥 ∈ R\n",
    "𝑁 ×𝑑in be the input, M ∈ R\n",
    "𝑑\n",
    "is the memory unit, and\n",
    "y ∈ R\n",
    "𝑑in is the output, then the general form of the recurrent neural network is defined as:\n",
    "M𝑡 = 𝑓 (M𝑡−1, 𝑥𝑡), Write Operation (6)\n",
    "y𝑡 = 𝑔(M𝑡\n",
    ", 𝑥𝑡), Read Operation (7)\n",
    "where 𝑓 (., .) is the read and 𝑔(., .) is the write corresponding functions. Note that here the subscript of M𝑡 shows the state\n",
    "of the memory at time 𝑡.\n",
    "In this perspective, the recurrence formula of linear Transformers (see Equation 4) is equivalent to additively compress\n",
    "and write keys and values, (𝐾𝑡\n",
    ",𝑉𝑡), into a matrix-valued memory unit M𝑡\n",
    ". Therefore, when dealing with long context\n",
    "data, this additive nature of the process results in memory overflow, significantly damaging the performance of the model.\n",
    "To address this, studies have focused on two promising directions: (1) Adding forget mechanism: several studies have\n",
    "presented adaptive (data-dependent) forgetting gate mechanisms for linear models, where it can erase the memory when it\n",
    "is needed. As examples of such models, we refer to GLA (S. Yang, B. Wang, Shen, et al. 2024), LRU (Orvieto et al. 2023),\n",
    "Griffin (De et al. 2024), xLSTM (Beck et al. 2024), and Mamba2 (Dao and Gu 2024), which the later is also connected to the\n",
    "discretized version of traditional state space models (Gu and Dao 2024).(2) Improving the write operation: To overcome the\n",
    "additive nature of memory write operation in traditional recurrent models, Widrow and Hoff (1988) presented Delta Rule,\n",
    "in which before adding a memory (i.e., a pair of key and value), the model first removes its past value. To enhance the\n",
    "parallelizable training and scaling, S. Yang, B. Wang, Yu Zhang, et al. (2024) present a fast paralellizable algorithm. Finally,\n",
    "very recently, S. Yang, Kautz, and Hatamizadeh (2024) improved the DeltaNets by adding a forget gate.\n",
    "Memory Modules. Memory has always been one of the core parts of the neural network designs (Graves, Wayne,\n",
    "and Danihelka 2014; JH Schmidhuber 1992; Jürgen Schmidhuber and Hochreiter 1997; J. Zhang et al. 2024). The idea of\n",
    "seeing linear layers as the key-value (associative) memory system backs to fast weight programs, in which dynamic fast\n",
    "programs are incorporated into recurrent neural networks to serve as writable memory (JH Schmidhuber 1992). The two\n",
    "learning rules of Hebbian (Hebb 2005) and delta (Prados and Kak 1989) are the most popular learning rules for fast weight\n",
    "programs, which have been extensively explored in various studies (Irie, Schlag, et al. 2021; Munkhdalai, Sordoni, et al.\n",
    "2019; Munkhdalai and H. Yu 2017; Schlag, Irie, and Jürgen Schmidhuber 2021; JH Schmidhuber 1992; S. Yang, Kautz, and\n",
    "Hatamizadeh 2024; S. Yang, B. Wang, Yu Zhang, et al. 2024). All these models, however, are based on momentary surprise,\n",
    "missing the token flow in the sequences (see Section 3.1), and most of them lacks a forgetting gate, resulting in a poor\n",
    "memory management.\n",
    "We further discuss the connection of our architectures with recent models in Appendix C. Additional related work are\n",
    "discussed in Appendix A.\n",
    "4\n",
    "3 Learning to Memorize at Test Time\n",
    "T\n",
    "o overcome the lack of long-term memory and to enable the model to learn, forget, and retrieve information, in\n",
    "this section, we present a neural long-term memory module, which is a meta models that learns to memorize at\n",
    "test time. In Section 3.1, we first discuss the motivation and the design of the neural memory. In Section 3.2, we\n",
    "discuss how our architecture design can benefit from a fast and parallelizable training. Finally, in Section 3.3, we augment\n",
    "our architecture using persistent memory module, in which we use learnable but data-independent parameters to learn\n",
    "meta information about the task.\n",
    "3.1 Long-term Memory\n",
    "To design a neural long-term memory module, we need a model that can encode the abstraction of the past history into its\n",
    "parameters. An example of this can be LLMs that are shown to be memorizing their training data (Leybzon and Kervadec\n",
    "2024; Schwarzschild et al. 2024; Staab et al. 2024). Therefore, a simple idea is to train a neural network and expect it to\n",
    "memorize its training data. Memorization, however, has almost always been known as an undesirable phenomena in\n",
    "neural networks as it limits the model generalization (Bayat et al. 2024), causes privacy concerns (Staab et al. 2024), and\n",
    "so results in poor performance at test time. Moreover, the memorization of the training data might not be helpful at test\n",
    "time, in which the data might be out-of-distribution. We argue that, we need an online meta-model that learns how to\n",
    "memorize/forget the data at test time. In this setup, the model is learning a function that is capable of memorization, but it\n",
    "is not overfitting to the training data, resulting in a better generalization at test time.\n",
    "Learning Process and Surprise Metric. The key idea to train a long-term memory is to treat its training as an online\n",
    "learning problem, in which we aim to compress the past information 𝑥1, . . . , 𝑥𝑡−1 into the parameters of our long-term\n",
    "neural memory module M𝑡\n",
    ". As discussed earlier, an event that violates the expectations (i.e., is surprising) is more\n",
    "memorable for humans (Mandler 2014). Inspired by this, a simple definition of surprise for a model can be its gradient with\n",
    "respect to the input. The larger the gradient is, the more different the input data is from the past data. Accordingly, using\n",
    "this surprise score, we can update the memory as:\n",
    "M𝑡 = M𝑡−1 − 𝜃𝑡 ∇ℓ(M𝑡−1; 𝑥𝑡)\n",
    "| {z }\n",
    "Surprise\n",
    ". (8)\n",
    "This surprise metric, however, can result in missing important information that comes after a big surprising moment.\n",
    "That is, the gradient can become extremely small after several surprising steps, leading to stocking in a flat area (i.e., local\n",
    "minima), and missing information about some parts of the sequence. From the human memory perspective, an event might\n",
    "not consistently surprise us through a long-period of time although it is memorable. The reason is that the initial moment\n",
    "is surprising enough to get our attention through a long time frame, leading to memorizing the entire time frame. To\n",
    "improve the above surprise metric (Equation 8), we break the surprise metric into (1) past surprise, which measures the\n",
    "surprise amount of a very recent past; and (2) momentary surprise, which measures the surprise of incoming data:\n",
    "M𝑡 = M𝑡−1 + 𝑆𝑡\n",
    ", (9)\n",
    "𝑆𝑡 = 𝜂𝑡 𝑆𝑡−1\n",
    "|{z}\n",
    "Past Surprise\n",
    "− 𝜃𝑡 ∇ℓ (𝑀𝑡−1; 𝑥𝑡)\n",
    "| {z }\n",
    "Momentary Surprise\n",
    ". (10)\n",
    "Interestingly, this formulation is similar to gradient descent with momentum, where 𝑆𝑡\n",
    "is the momentum element. Therefore,\n",
    "the momentum here act as a memory of surprise across time (sequence length). In this formulation, the term 𝜂𝑡\n",
    "is a\n",
    "data-dependent surprise decay (a function of 𝑥𝑡\n",
    "), controlling how surprise decays over time, and the term 𝜃𝑡\n",
    "is controlling\n",
    "how much of momentary surprise should be incorporated into the final surprise metric in a data-dependent manner. This\n",
    "data-dependency is particularly important in this design: While surprise of previous tokens might be needed to affect\n",
    "the surprise of the next token, it is mostly valid if all tokens are relevant and are in the same context. Accordingly, a\n",
    "data-dependent 𝜂 can control if memory needs to: (1) ignore the last surprise by setting 𝜂𝑡 → 0 (possibly due to the change\n",
    "of context), or (2) fully incorporate the last surprise by setting 𝜂𝑡 → 1 (possibly as the token is highly relevant to its recent\n",
    "past tokens).\n",
    "Objective. Our above surprise metric is based on a loss function ℓ(.; .), which is the objective that our memory is learning\n",
    "to act as it at test time. That is, our memory module is a meta model that learns a function based on the loss function ℓ(.; .).\n",
    "5\n",
    "In this work, we focus on associative memory, in which we aim to store the past data as the pairs of keys and values. Given\n",
    "𝑥𝑡\n",
    ", similar to Transformers (Vaswani et al. 2017), we use two linear layers to project 𝑥𝑡\n",
    "into a key and value:\n",
    "k𝑡 = 𝑥𝑡𝑊𝐾, v𝑡 = 𝑥𝑡𝑊𝑉 , (11)\n",
    "where 𝑊𝐾 and 𝑊𝑉 ∈ R\n",
    "𝑑in×𝑑in . Next, we expect our memory module to learn the associations between keys and values. To\n",
    "this end, we define the loss as follows:\n",
    "ℓ(M𝑡−1; 𝑥𝑡) = ∥M𝑡−1 (k𝑡) − v𝑡 ∥\n",
    "2\n",
    "2\n",
    "(12)\n",
    "By optimizing the above loss function in the inner-loop of our meta model (memory), the model learns how to memorize\n",
    "the mapping between keys and values at test time. Note that, similar to meta-learning models (Nichol 2018; Zintgraf et al.\n",
    "2019), training of the memory is in the inner-loop, and so parameters 𝑊𝐾 and 𝑊𝑉 are hyperparameters in the above loss\n",
    "function. Accordingly, in the inner loop, we optimize M’s weights, while in the outer-loop, we optimize other parameters\n",
    "of the entire architecture.\n",
    "Forgetting Mechanism. When dealing with very large sequences (e.g., millions of tokens), it is crucial to manage which\n",
    "past information should be forgotten–even with a deep or a very large matrix-valued memory. To this end, we use an\n",
    "adaptive forgetting mechanism that allows the memory to forget the information that is not needed anymore, resulting in\n",
    "better managing the memory’s limited capacity. That is, given the next token 𝑥𝑡\n",
    ", we modify the update rule as:\n",
    "M𝑡 = (1 − 𝛼𝑡)M𝑡−1 + 𝑆𝑡\n",
    ", (13)\n",
    "𝑆𝑡 = 𝜂𝑡𝑆𝑡−1 − 𝜃𝑡 ∇ℓ (𝑀𝑡−1; 𝑥𝑡), (14)\n",
    "where 𝛼𝑡 ∈ [0, 1] is the gating mechanism that flexibly controls the memory; i.e., decides how much information should be\n",
    "forgotten. For example, it can update the memory without affecting the past abstraction by letting 𝛼𝑡 → 0, and can clear\n",
    "the entire memory by letting 𝛼𝑡 → 1. Later in this section, we show that this weight decay mechanism is closely related to\n",
    "the gating mechanism in modern RNNs (Dao and Gu 2024; Orvieto et al. 2023).\n",
    "Memory Architecture. In this paper, we focus on simple MLPs with 𝐿M ≥ 1 layers as the architecture of our long-term\n",
    "memory. The main reason behind this choice is that we want to focus on better motivating the design of the long-term\n",
    "memory and ways that it can be incorporated into an architecture. However, our formulation and architectural design\n",
    "opens a new research direction to design neural architectures that are more effective and efficient in memorization of data.\n",
    "Recently, there has been a promising line of work to design such architectures (Berges et al. 2024; Cetin et al. 2024; J. Zhang\n",
    "et al. 2024), which incorporating them into our framework (i.e., replacing simple MLPs with such architectures) can be an\n",
    "interesting future work.\n",
    "When using vector-valued or matrix-valued memory (De et al. 2024; Orvieto et al. 2023; S. Yang, B. Wang, Shen, et\n",
    "al. 2024), the memory module is compressing the past data and fit it into a line. That is, from the meta learning or\n",
    "online learning perspective (Yu Sun et al. 2024), using a matrix-valued memory M = 𝑊 ∈ R\n",
    "𝑑in×𝑑in is equivalent to\n",
    "optimize ℓ(𝑊𝑡−1; 𝑥𝑡) = ∥𝑊𝑡−1k𝑡 − v𝑡 ∥\n",
    "2\n",
    "2\n",
    ", which is an online linear regression objective and so the optimal solution assumes\n",
    "the underlying dependency of historical data is linear. On the other hand, we argue that deep memory modules (i.e.,\n",
    "𝐿M ≥ 2) . Aligning with the theoretical results that MLPs with at least two layers are strictly more expressive than linear\n",
    "models (Hornik, Stinchcombe, and White 1989), in Section 5.5, we show that deep memory modules are more effective in\n",
    "practice.\n",
    "Retrieving a Memory. In the above, we discuss how one can design and train a long-term memory module that learns to\n",
    "memorize at test time. A key remaining question is: How one can retrieve information from the memory? We simply use the\n",
    "forward pass without weight update (i.e., inference) to retrieve a memory correspond to a query. Formally, given an input\n",
    "𝑥𝑡\n",
    ", we use a linear layer 𝑊𝑄 to project the input, i.e., q𝑡 = 𝑥𝑡𝑊𝑄 and retrieve the corresponding (or useful) information\n",
    "from the memory 𝑦𝑡 by:\n",
    "𝑦𝑡 = M∗\n",
    "(q𝑡). (15)\n",
    "6\n",
    "Figure 1: The illustration of how the training of neural memory can be done in parallel and using matmuls.\n",
    "3.2 How to Parallelize the Long-term Memory Training\n",
    "As discussed above, the design of our long-term memory module is equivalent to training a meta model by optimizing\n",
    "associative memory loss function ℓ(M𝑡−1; 𝑥𝑡) = ∥M𝑡−1 (k𝑡) − v𝑡 ∥\n",
    "2\n",
    "2 using gradient descent with momentum and weight\n",
    "decay. Therefore, in theory, the training of long-term memory module requires O (𝑁) FLOPs, where 𝑁 is the sequence\n",
    "length. However, in practice, we need to parallelize the training process and to fully take advantage of hardware accelerators\n",
    "(e.g., TPUs, GPUs), we need to tensorize the process and use more matmuls.\n",
    "Next, we show that calculating the weights in the inner loop with mini-batch gradient descent, data-dependent learning\n",
    "rate, and weight decay can be reformulated so that it uses only matmuls and sum. We build upon the work of Yu Sun et al.\n",
    "(2024) that shows forward pass of a model optimizing with the mini-batch gradient descent (with constant learning rate)\n",
    "can be calculated using matmuls. We can split the sequence into chunks of size 𝑏 ≥ 1, and write the mini-batch gradient\n",
    "descent as:\n",
    "M𝑡 = (1 − 𝛼𝑡)M𝑡−1 − 𝜃𝑡∇ℓ(M𝑡−1; 𝑥𝑡) = 𝛽𝑡M0 −\n",
    "∑︁𝑡\n",
    "𝑖=1\n",
    "𝜃𝑖\n",
    "𝛽𝑡\n",
    "𝛽𝑖\n",
    "∇ℓ(M𝑡\n",
    "′ ; 𝑥𝑖), (16)\n",
    "where 𝑡\n",
    "′ = 𝑡 − mod(𝑡, 𝑏), and 𝛽𝑖 =\n",
    "Î𝑖\n",
    "𝑗=1\n",
    "(1 − 𝛼𝑗). For the sake of simplicity, we focus on the first chunk, i.e., 𝑡 = 𝑏 and so\n",
    "𝑡\n",
    "′ = 0. Also, we explain the process for the case that M𝑡 = 𝑊𝑡\n",
    "is linear. The process for MLPs with 𝑁𝑝 ≥ 2 is similar. Using\n",
    "our loss function, we have:\n",
    "∇ℓ(𝑊0; 𝑥𝑡) = (𝑊0𝑥𝑡 − 𝑥𝑡)𝑥\n",
    "⊤\n",
    "𝑡 ⇒\n",
    "∑︁\n",
    "𝑏\n",
    "𝑖=1\n",
    "𝜃𝑖\n",
    "𝛽𝑏\n",
    "𝛽𝑖\n",
    "∇ℓ(𝑊0; 𝑥𝑖) = Θ𝑏B𝑏 (𝑊0𝑋 − 𝑋)𝑋\n",
    "⊤\n",
    ", (17)\n",
    "where Θ𝑏 = diag\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I need to create a set of 10 multiple-choice quiz questions based on the resume provided by the user. The task is to test their knowledge and skills outlined in the text.\n",
      "\n",
      "First, let me understand what the user wants. They have written a detailed description about a resume or job application, which includes bullet points, explanations for each skill, and some additional notes like important dates or considerations. Now, they want an interview quiz with 10 questions that test these skills.\n",
      "\n",
      "I should focus on the key areas mentioned: leadership, problem-solving, communication, time management, teamwork, data analysis, adaptability, technical knowledge (specifically Python), and emotional intelligence. Each of these areas has subtopics that I can ask about in the quiz.\n",
      "\n",
      "For each question, I need to create a clear and concise option for the answer with four choices. The correct answer should be evident from the context provided in the resume. Let's go through each skill:\n",
      "\n",
      "1. **Leadership**: Focus on strategies or techniques. Maybe a question about recognizing effective leadership styles.\n",
      "2. **Problem-solving**: Test their ability to analyze issues and come up with solutions.\n",
      "3. **Communication**: Questions could involve clarity, clarity vs. conciseness, or active listening.\n",
      "4. **Time Management**: Focus on planning and prioritizing tasks efficiently.\n",
      "5. **Teamwork**: Questions about collaboration, decision-making, role understanding, and conflict resolution.\n",
      "6. **Data Analysis**: Tests their ability to interpret data, draw conclusions, suggest improvements, and identify trends.\n",
      "7. **Adaptability**: Questions about adjusting strategies in changing environments, learning from challenges, resilience, or accepting criticism.\n",
      "8. **Technical Knowledge**: Specific topics like Python syntax, libraries, frameworks, etc., depending on the job description.\n",
      "9. **Emotional Intelligence**: Focus on emotional regulation, empathy, positive self-talk, and handling stress.\n",
      "10. **Other Skills (if applicable)**: Questions about creativity, responsibility, innovation, or other relevant traits.\n",
      "\n",
      "I need to ensure each question is clear, the options are plausible but not obvious, and the correct answer stands out. Let me structure each question accordingly, ensuring they are varied in difficulty and cover different aspects of the resume skills.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"title\": \"Intelligence Quiz: Resume Skills Assessment\",\n",
      "    \"questions\": [\n",
      "        {\n",
      "            \"question\": \"Which is NOT a leadership strategy for motivating employees?\",\n",
      "            \"choices\": [\n",
      "                \"Acknowledge employees' input regularly.\",\n",
      "                \"Push employees to take initiative in decisions.\",\n",
      "                \"Celebrate team successes and recognize effort.\",\n",
      "                \"Implement a rigid hierarchical structure.\"\n",
      "            ],\n",
      "            \"answer\": \"Celebrate team successes and recognize effort.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can one improve their problem-solving approach?\",\n",
      "            \"choices\": [\n",
      "                \"Approach problems passively while observing others.\",\n",
      "                \"Identify issues by questioning the source of the problem.\",\n",
      "                \"Solve a problem, then generalize it to other areas.\",\n",
      "                \"Use a trial-and-error method first.\",\n",
      "            ],\n",
      "            \"answer\": \"Approach problems passively while observing others.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is effective communication in workplace settings?\",\n",
      "            \"choices\": [\n",
      "                \"Emphasize only the positive aspects of an employee's behavior.\",\n",
      "                \"Repetition and repetition: The key to good communication.\",\n",
      "                \"Acknowledge employees' differences but avoid criticism.\",\n",
      "                \"Focus on technical expertise over soft skills.\",\n",
      "            ],\n",
      "            \"answer\": \"Emphasize only the positive aspects of an employee's behavior.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can one optimize their time management strategy?\",\n",
      "            \"choices\": [\n",
      "                \"List tasks in order of priority and then task type.\",\n",
      "                \"Prioritize by complexity, not importance.\",\n",
      "                \"Prioritize by urgency, not by impact.\",\n",
      "                \"Prioritize based on job role rather than personal interest.\",\n",
      "            ],\n",
      "            \"answer\": \"List tasks in order of priority and then task type.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is important when working with a team to reach consensus?\",\n",
      "            \"choices\": [\n",
      "                \"Approach the problem from multiple angles simultaneously.\",\n",
      "                \"Agree on a single solution after discussing each issue.\",\n",
      "                \"Listen without interrupting, focusing instead on your own ideas.\",\n",
      "                \"Embrace resistance and let it become part of the decision-making process.\",\n",
      "            ],\n",
      "            \"answer\": \"Approach the problem from multiple angles simultaneously.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"Which skill is crucial for effective data analysis in a job role?\",\n",
      "            \"choices\": [\n",
      "                \"Strong ability to interpret project reports accurately.\",\n",
      "                \"Analyzing raw data and interpreting trends without context.\",\n",
      "                \"Assessing data quality and preparing for the worst case scenario.\",\n",
      "                \"Developing new analytical frameworks that address unique challenges.\",\n",
      "            ],\n",
      "            \"answer\": \"Strong ability to interpret project reports accurately.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can one improve adaptability in a fast-evolving environment?\",\n",
      "            \"choices\": [\n",
      "                \"Adopt the current approach and pivot quickly when needed.\",\n",
      "                \"Focus on learning from past mistakes before adapting.\",\n",
      "                \"Accept criticism and seek ways to address it immediately.\",\n",
      "                \"Preemptively plan for future changes and unexpected challenges.\",\n",
      "            ],\n",
      "            \"answer\": \"Adopt the current approach and pivot quickly when needed.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is key to effective technical knowledge in a job role?\",\n",
      "            \"choices\": [\n",
      "                \"Gaining technical expertise through hands-on practice after initial learning.\",\n",
      "                \"Mastering concepts and tools by reading documentation first.\",\n",
      "                \"Leveraging existing frameworks and libraries without delving into underlying code.\",\n",
      "                \"Understanding how different technologies interact and integrate seamlessly.\",\n",
      "            ],\n",
      "            \"answer\": \"Gaining technical expertise through hands-on practice after initial learning.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"How can one enhance emotional intelligence in a workplace setting?\",\n",
      "            \"choices\": [\n",
      "                \"Practice deep self-reflection and avoid engaging with others during stress times.\",\n",
      "                \"Acknowledge the emotions of your co-workers without judgment.\",\n",
      "                \"Take control of your own emotional reactions, regardless of the situation.\",\n",
      "                \"Strive to make others feel good rather than force them to feel good.\",\n",
      "            ],\n",
      "            \"answer\": \"Practice deep self-reflection and avoid engaging with others during stress times.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is important when handling unexpected situations in a team setting?\",\n",
      "            \"choices\": [\n",
      "                \"Take initiative to address the issue immediately after it occurs.\",\n",
      "                \"Neglect to communicate concerns to higher-ups when needed.\",\n",
      "                \"Prepare to escalate or resolve the issue before presenting to the team.\",\n",
      "                \"Embrace compromise and work collaboratively without holding firm on your part.\",\n",
      "            ],\n",
      "            \"answer\": \"Take initiative to address the issue immediately after it occurs.\"\n",
      "        },\n",
      "        {\n",
      "            \"question\": \"What is key for effective communication in technical roles?\",\n",
      "            \"choices\": [\n",
      "                \"Be clear and concise when communicating technical information.\",\n",
      "                \"Avoid using jargon or overly complex terms where simpler terms suffice.\",\n",
      "                \"Leverage analogies to make technical concepts more relatable.\",\n",
      "                \"Present solutions in the simplest way possible without missing any critical points.\",\n",
      "            ],\n",
      "            \"answer\": \"Be clear and concise when communicating technical information.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1:1.5b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': human_text + '\\n' + sys_context,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n{\\n    \"title\": \"Understanding Neural Memory and Training\",\\n    \"questions\": [\\n        {\\n            \"question\": \"What is the capital of France?\",\\n            \"choices\": [\\n                \"Paris\",\\n                \"London\",\\n                \"Berlin\",\\n                \"Madrid\"\\n            ],\\n            \"answer\": \"Paris\"\\n        },\\n        {\\n            \"question\": \"Which derivative of the ReLU activation function is used in neural networks?\",\\n            \"choices\": [\\n                \"Sigmoid\",\\n                \"Leaky Rectified Linear Unit (ReLU)\",\\n                \"Hard Sigmoid\",\\n                \"Tanh\"\\n            ],\\n            \"answer\": \"Leaky Rectified Linear Unit (ReLU)\"\\n        },\\n        {\\n            \"question\": \"The associative memory loss function used in neural memory is formulated as?\",\\n            \"choices\": [\\n                \"||M(t-1) * k(t) - v(t)||^2\",\\n                \"M(t-1) * k(t) - v(t)\",\\n                \"k(t) - M(t-1) * v(t)\",\\n                \"k(t) / (M(t-1) + v(t))\"\\n            ],\\n            \"answer\": \"||M(t-1) * k(t) - v(t)||^2\"\\n        },\\n        {\\n            \"question\": \"During the training process, if a batch of data 𝑥 = {x₁, x₂, ..., x_b} is processed sequentially with mini-batch GD and weight decay, how does the weight update formula simplify?\",\\n            \"choices\": [\\n                \"(1 - α_t) * M_{t-1} - Σ(θ_i / β_i) * ∇ℓ(M_{t\\'}, x_i)\",\\n                \"∑(β_i / (1 - α_i)) * gradient terms\"\\n            ],\\n            \"answer\": \"(1 - α_t) * M_{t-1} - ∑(θ_i / β_i) * ∇ℓ(M_{t\\'}, x_i)\"\\n        },\\n        {\\n            \"question\": \"Multi-layer neural networks are more effective than linear models because they can handle complex relationships due to their layered structure.\",\\n            \"choices\": [\\n                \"True\",\\n                \"False\"\\n            ],\\n            \"answer\": \"True\"\\n        },\\n        {\\n            \"question\": \"When using data-dependent learning rates and momentum in gradient descent, the weight update formula transforms into a form that leverages matrix operations for efficient computation.\",\\n            \"choices\": [\\n                \"Only linear updates with fixed steps\",\\n                \"Matrix-based updates with dynamic steps based on data\"\\n            ],\\n            \"answer\": \"Matrix-based updates with dynamic steps based on data\"\\n        },\\n        {\\n            \"question\": \"The effectiveness of multi-layer networks is supported by the fact that they are strictly more expressive than linear models.\",\\n            \"choices\": [\\n                \"True\",\\n                \"False\"\\n            ],\\n            \"answer\": \"True\"\\n        },\\n        {\\n            \"question\": \"Differentiating the loss function using matrix operations shows how each weight\\'s contribution to the overall error is quantified in a structured manner.\",\\n            \"choices\": [\\n                \"Single-variable differentiation\",\\n                \"Multi-variable differentiation with partial derivatives\"\\n            ],\\n            \"answer\": \"Multi-variable differentiation with partial derivatives\"\\n        },\\n        {\\n            \"question\": \"In the forward pass derivation of the loss function, matrix operations are used to model how each data point contributes to the overall network\\'s output.\",\\n            \"choices\": [\\n                \"Single-layer matrix multiplication\",\\n                \"Two-layer matrix multiplication including weights and biases\"\\n            ],\\n            \"answer\": \"Two-layer matrix multiplication including weights and biases\"\\n        },\\n        {\\n            \"question\": \"The formula for differentiating the loss function using matrix operations is essential because it allows for efficient computation of gradients during backpropagation.\",\\n            \"choices\": [\\n                \"True\",\\n                \"False\"\\n            ],\\n            \"answer\": \"True\"\\n        }\\n    ]\\n}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r.split(\"</think>\")[1]\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{     \"title\": \"Understanding Neural Memory and Training\",     \"questions\": [         {             \"question\": \"What is the capital of France?\",             \"choices\": [                 \"Paris\",                 \"London\",                 \"Berlin\",                 \"Madrid\"             ],             \"answer\": \"Paris\"         },         {             \"question\": \"Which derivative of the ReLU activation function is used in neural networks?\",             \"choices\": [                 \"Sigmoid\",                 \"Leaky Rectified Linear Unit (ReLU)\",                 \"Hard Sigmoid\",                 \"Tanh\"             ],             \"answer\": \"Leaky Rectified Linear Unit (ReLU)\"         },         {             \"question\": \"The associative memory loss function used in neural memory is formulated as?\",             \"choices\": [                 \"||M(t-1) * k(t) - v(t)||^2\",                 \"M(t-1) * k(t) - v(t)\",                 \"k(t) - M(t-1) * v(t)\",                 \"k(t) / (M(t-1) + v(t))\"             ],             \"answer\": \"||M(t-1) * k(t) - v(t)||^2\"         },         {             \"question\": \"During the training process, if a batch of data 𝑥 = {x₁, x₂, ..., x_b} is processed sequentially with mini-batch GD and weight decay, how does the weight update formula simplify?\",             \"choices\": [                 \"(1 - α_t) * M_{t-1} - Σ(θ_i / β_i) * ∇ℓ(M_{t\\'}, x_i)\",                 \"∑(β_i / (1 - α_i)) * gradient terms\"             ],             \"answer\": \"(1 - α_t) * M_{t-1} - ∑(θ_i / β_i) * ∇ℓ(M_{t\\'}, x_i)\"         },         {             \"question\": \"Multi-layer neural networks are more effective than linear models because they can handle complex relationships due to their layered structure.\",             \"choices\": [                 \"True\",                 \"False\"             ],             \"answer\": \"True\"         },         {             \"question\": \"When using data-dependent learning rates and momentum in gradient descent, the weight update formula transforms into a form that leverages matrix operations for efficient computation.\",             \"choices\": [                 \"Only linear updates with fixed steps\",                 \"Matrix-based updates with dynamic steps based on data\"             ],             \"answer\": \"Matrix-based updates with dynamic steps based on data\"         },         {             \"question\": \"The effectiveness of multi-layer networks is supported by the fact that they are strictly more expressive than linear models.\",             \"choices\": [                 \"True\",                 \"False\"             ],             \"answer\": \"True\"         },         {             \"question\": \"Differentiating the loss function using matrix operations shows how each weight\\'s contribution to the overall error is quantified in a structured manner.\",             \"choices\": [                 \"Single-variable differentiation\",                 \"Multi-variable differentiation with partial derivatives\"             ],             \"answer\": \"Multi-variable differentiation with partial derivatives\"         },         {             \"question\": \"In the forward pass derivation of the loss function, matrix operations are used to model how each data point contributes to the overall network\\'s output.\",             \"choices\": [                 \"Single-layer matrix multiplication\",                 \"Two-layer matrix multiplication including weights and biases\"             ],             \"answer\": \"Two-layer matrix multiplication including weights and biases\"         },         {             \"question\": \"The formula for differentiating the loss function using matrix operations is essential because it allows for efficient computation of gradients during backpropagation.\",             \"choices\": [                 \"True\",                 \"False\"             ],             \"answer\": \"True\"         }     ] }'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3 = r2.strip().replace(\"\\n\", \" \")\n",
    "r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Understanding Neural Memory and Training',\n",
       " 'questions': [{'question': 'What is the capital of France?',\n",
       "   'choices': ['Paris', 'London', 'Berlin', 'Madrid'],\n",
       "   'answer': 'Paris'},\n",
       "  {'question': 'Which derivative of the ReLU activation function is used in neural networks?',\n",
       "   'choices': ['Sigmoid',\n",
       "    'Leaky Rectified Linear Unit (ReLU)',\n",
       "    'Hard Sigmoid',\n",
       "    'Tanh'],\n",
       "   'answer': 'Leaky Rectified Linear Unit (ReLU)'},\n",
       "  {'question': 'The associative memory loss function used in neural memory is formulated as?',\n",
       "   'choices': ['||M(t-1) * k(t) - v(t)||^2',\n",
       "    'M(t-1) * k(t) - v(t)',\n",
       "    'k(t) - M(t-1) * v(t)',\n",
       "    'k(t) / (M(t-1) + v(t))'],\n",
       "   'answer': '||M(t-1) * k(t) - v(t)||^2'},\n",
       "  {'question': 'During the training process, if a batch of data 𝑥 = {x₁, x₂, ..., x_b} is processed sequentially with mini-batch GD and weight decay, how does the weight update formula simplify?',\n",
       "   'choices': [\"(1 - α_t) * M_{t-1} - Σ(θ_i / β_i) * ∇ℓ(M_{t'}, x_i)\",\n",
       "    '∑(β_i / (1 - α_i)) * gradient terms'],\n",
       "   'answer': \"(1 - α_t) * M_{t-1} - ∑(θ_i / β_i) * ∇ℓ(M_{t'}, x_i)\"},\n",
       "  {'question': 'Multi-layer neural networks are more effective than linear models because they can handle complex relationships due to their layered structure.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'},\n",
       "  {'question': 'When using data-dependent learning rates and momentum in gradient descent, the weight update formula transforms into a form that leverages matrix operations for efficient computation.',\n",
       "   'choices': ['Only linear updates with fixed steps',\n",
       "    'Matrix-based updates with dynamic steps based on data'],\n",
       "   'answer': 'Matrix-based updates with dynamic steps based on data'},\n",
       "  {'question': 'The effectiveness of multi-layer networks is supported by the fact that they are strictly more expressive than linear models.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'},\n",
       "  {'question': \"Differentiating the loss function using matrix operations shows how each weight's contribution to the overall error is quantified in a structured manner.\",\n",
       "   'choices': ['Single-variable differentiation',\n",
       "    'Multi-variable differentiation with partial derivatives'],\n",
       "   'answer': 'Multi-variable differentiation with partial derivatives'},\n",
       "  {'question': \"In the forward pass derivation of the loss function, matrix operations are used to model how each data point contributes to the overall network's output.\",\n",
       "   'choices': ['Single-layer matrix multiplication',\n",
       "    'Two-layer matrix multiplication including weights and biases'],\n",
       "   'answer': 'Two-layer matrix multiplication including weights and biases'},\n",
       "  {'question': 'The formula for differentiating the loss function using matrix operations is essential because it allows for efficient computation of gradients during backpropagation.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Understanding Neural Memory and Training',\n",
       " 'questions': [{'question': 'What is the capital of France?',\n",
       "   'choices': ['Paris', 'London', 'Berlin', 'Madrid'],\n",
       "   'answer': 'Paris'},\n",
       "  {'question': 'Which derivative of the ReLU activation function is used in neural networks?',\n",
       "   'choices': ['Sigmoid',\n",
       "    'Leaky Rectified Linear Unit (ReLU)',\n",
       "    'Hard Sigmoid',\n",
       "    'Tanh'],\n",
       "   'answer': 'Leaky Rectified Linear Unit (ReLU)'},\n",
       "  {'question': 'The associative memory loss function used in neural memory is formulated as?',\n",
       "   'choices': ['||M(t-1) * k(t) - v(t)||^2',\n",
       "    'M(t-1) * k(t) - v(t)',\n",
       "    'k(t) - M(t-1) * v(t)',\n",
       "    'k(t) / (M(t-1) + v(t))'],\n",
       "   'answer': '||M(t-1) * k(t) - v(t)||^2'},\n",
       "  {'question': 'During the training process, if a batch of data 𝑥 = {x₁, x₂, ..., x_b} is processed sequentially with mini-batch GD and weight decay, how does the weight update formula simplify?',\n",
       "   'choices': [\"(1 - α_t) * M_{t-1} - Σ(θ_i / β_i) * ∇ℓ(M_{t'}, x_i)\",\n",
       "    '∑(β_i / (1 - α_i)) * gradient terms'],\n",
       "   'answer': \"(1 - α_t) * M_{t-1} - ∑(θ_i / β_i) * ∇ℓ(M_{t'}, x_i)\"},\n",
       "  {'question': 'Multi-layer neural networks are more effective than linear models because they can handle complex relationships due to their layered structure.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'},\n",
       "  {'question': 'When using data-dependent learning rates and momentum in gradient descent, the weight update formula transforms into a form that leverages matrix operations for efficient computation.',\n",
       "   'choices': ['Only linear updates with fixed steps',\n",
       "    'Matrix-based updates with dynamic steps based on data'],\n",
       "   'answer': 'Matrix-based updates with dynamic steps based on data'},\n",
       "  {'question': 'The effectiveness of multi-layer networks is supported by the fact that they are strictly more expressive than linear models.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'},\n",
       "  {'question': \"Differentiating the loss function using matrix operations shows how each weight's contribution to the overall error is quantified in a structured manner.\",\n",
       "   'choices': ['Single-variable differentiation',\n",
       "    'Multi-variable differentiation with partial derivatives'],\n",
       "   'answer': 'Multi-variable differentiation with partial derivatives'},\n",
       "  {'question': \"In the forward pass derivation of the loss function, matrix operations are used to model how each data point contributes to the overall network's output.\",\n",
       "   'choices': ['Single-layer matrix multiplication',\n",
       "    'Two-layer matrix multiplication including weights and biases'],\n",
       "   'answer': 'Two-layer matrix multiplication including weights and biases'},\n",
       "  {'question': 'The formula for differentiating the loss function using matrix operations is essential because it allows for efficient computation of gradients during backpropagation.',\n",
       "   'choices': ['True', 'False'],\n",
       "   'answer': 'True'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
